{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# @param dfFile: pandas.DataFrame ('nba_preprocessed.csv')\n",
    "# @param dateStart, dateEnd: str in the format of 'YYYY-MM-DD'\n",
    "# @param attriToDrop: list[str]\n",
    "# @return X, Y: pandas.DataFrame\n",
    "# featureExtraction() outputs X, Y for model training.\n",
    "# Game date can be assigned\n",
    "# Attribute to be dropped can be assigned\n",
    "def featureExtraction(dfFile, dateStart='1000-01-01', dateEnd='2999-12-31', attriToDrop=None):\n",
    "    df = pd.read_csv(dfFile)\n",
    "    \n",
    "    # Date selection\n",
    "    df = df.loc[(df.Date_A > dateStart) & (df.Date_A < dateEnd), :].reset_index(drop=True)\n",
    "    \n",
    "    # Get label Y\n",
    "    Y = df[['W/L_A']]\n",
    "    Y = Y.rename(columns={'W/L_A': 'Label'})\n",
    "    \n",
    "    # Get attributes X\n",
    "    attriToDrop = [x + '_A' for x in attriToDrop] + [x + '_B' for x in attriToDrop] if attriToDrop else []\n",
    "    colToDrop = ['Team_A', 'Date_A', 'W/L_A', 'Score_A', 'Opponent_A', 'Team_B', 'Date_B', 'W/L_B', 'Home/Away_B', 'Score_B', 'Opponent_B']\n",
    "    colToDrop += attriToDrop if attriToDrop else []\n",
    "    X = df.drop(columns = colToDrop)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "def StackingMethod(origin_df_X, origin_df_Y, kfold, is_debug, **all_basic_classifiers):\n",
    "    X_train = origin_df_X.values\n",
    "    Y_train = origin_df_Y.values.ravel()\n",
    "    random.seed(datetime.now())\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=random.randint(0, 2**32-1), shuffle=True)\n",
    "    iteration = 0\n",
    "    len_y = 0\n",
    "    new_feature_columns = ['Label_'+x for x in all_basic_classifiers.keys()]\n",
    "    new_feature_arr     = np.zeros([len(X_train), len(new_feature_columns)])\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_train, Y_train):\n",
    "        X_cv_train = X_train[train_index]\n",
    "        Y_cv_train = Y_train[train_index]\n",
    "        X_cv_test  = X_train[test_index]\n",
    "        Y_cv_test  = Y_train[test_index]\n",
    "        column_label_index = 0\n",
    "        \n",
    "        if(is_debug):\n",
    "            print(f\"-----iteration {iteration}-----\")\n",
    "            print(f'test_index = {test_index}')\n",
    "        for k, v in all_basic_classifiers.items():\n",
    "            classifier_cv = v\n",
    "            classifier_cv.fit(X_cv_train, Y_cv_train)\n",
    "            Y_cv_test_result = classifier_cv.predict(X_cv_test)\n",
    "            count_result_index = 0\n",
    "            for index in test_index:\n",
    "                new_feature_arr[index][column_label_index] = Y_cv_test_result[count_result_index]\n",
    "                count_result_index += 1\n",
    "                \n",
    "            column_label_index += 1\n",
    "            \n",
    "            if(is_debug):\n",
    "                len_y += len(Y_cv_test_result)\n",
    "                print(f'key = {k}, val = {v}')\n",
    "                print(f'Y_cv_test_result = {Y_cv_test_result}')\n",
    "                print(f'len(Y_cv_test_result) = {len(Y_cv_test_result)}')\n",
    "                print(type(Y_cv_test_result))\n",
    "                print('-------')\n",
    "        iteration += 1\n",
    "        \n",
    "    new_feature_df = pd.DataFrame(data = new_feature_arr, columns = new_feature_columns)\n",
    "    output_df_all = pd.concat([origin_df_X, new_feature_df], axis=1, ignore_index=False)\n",
    "    \n",
    "    if(is_debug):\n",
    "        print(f'total len_y = {len_y}')\n",
    "        print(f'new_feature_columns = {new_feature_columns}')\n",
    "        count_index_fin = 0\n",
    "        for x in new_feature_arr:\n",
    "            print(f'index = {count_index_fin}, label = {x}')\n",
    "            count_index_fin += 1\n",
    "        \n",
    "    return output_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossValidationGridSearchNested(origin_df_X, origin_df_Y, num_trials, fold_num, est_classifcation, tuned_param, scoring):\n",
    "    X_data = origin_df_X.values\n",
    "    Y_data = origin_df_Y.values.ravel()\n",
    "    max_score = -1\n",
    "    best_estimator = est_classifcation\n",
    "    is_tuned_param_empty = (tuned_param == []) | (tuned_param == None)\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        inner_cv = StratifiedKFold(n_splits=fold_num, random_state=i, shuffle=True)\n",
    "        outer_cv = StratifiedKFold(n_splits=fold_num, random_state=i+1, shuffle=True)\n",
    "        \n",
    "        if(is_tuned_param_empty):\n",
    "            param_score = cross_val_score(est_classifcation, X=X_data, y=Y_data, cv=outer_cv, scoring=scoring).mean()\n",
    "        else:\n",
    "            # Non_nested parameter search and scoring\n",
    "            clf = GridSearchCV(estimator=est_classifcation, param_grid=tuned_param, cv=inner_cv, scoring=scoring)\n",
    "            clf.fit(X_data, Y_data)\n",
    "        \n",
    "            # CV with parameter optimization\n",
    "            param_score = cross_val_score(clf.best_estimator_, X=X_data, y=Y_data, cv=outer_cv, scoring=scoring).mean()\n",
    "            \n",
    "        if(param_score > max_score):\n",
    "            max_score = param_score\n",
    "            if(is_tuned_param_empty):\n",
    "                best_estimator = est_classifcation\n",
    "            else:\n",
    "                best_estimator = clf.best_estimator_\n",
    "            \n",
    "        progress = (i+1)/num_trials*100\n",
    "        print(f'> progress = {progress}%')\n",
    "    \n",
    "    return (max_score, best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Only support roc_auc scoring now\n",
    "def CrossValidationGetModelsScores(origin_df_X, origin_df_Y, num_trials, fold_num, est_classifier):\n",
    "    X_data = origin_df_X.values\n",
    "    Y_data = origin_df_Y.values.ravel()\n",
    "    random.seed(datetime.now())\n",
    "    skf = StratifiedKFold(n_splits=fold_num, random_state=random.randint(0, 2**32-1), shuffle=True)\n",
    "    ret_n_estimator = []\n",
    "    ret_n_score     = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_data, Y_data):\n",
    "        X_cv_train = X_data[train_index]\n",
    "        Y_cv_train = Y_data[train_index]\n",
    "        X_cv_test  = X_data[test_index]\n",
    "        Y_cv_test  = Y_data[test_index]\n",
    "        \n",
    "        ret_n_estimator.append(est_classifier)\n",
    "        ret_n_estimator[-1].fit(X_cv_train, Y_cv_train)\n",
    "        ret_n_score.append(roc_auc_score(Y_cv_test, ret_n_estimator[-1].predict(X_cv_test)))\n",
    "    \n",
    "    return(ret_n_estimator, ret_n_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_1d_test_array is the 1-dimension test array\n",
    "def PredictFunctionAggregation(x_1d_test_array, n_estimator, voting=1, is_debug=0):\n",
    "    ret_val = 0\n",
    "    if(voting == 1):\n",
    "        y_predict = [x.predict(x_1d_test_array) for x in n_estimator]\n",
    "        count_0 = y_predict.count(0)\n",
    "        count_1 = y_predict.count(1)\n",
    "        \n",
    "        if(is_debug):\n",
    "            print(f'y_predict = {y_predict}')\n",
    "            print(f'count_0   = {count_0}')\n",
    "            print(f'count_1   = {count_1}')\n",
    "\n",
    "        if(count_0 > count_1):\n",
    "            ret_val = 0\n",
    "        else:\n",
    "            ret_val = 1\n",
    "    else:\n",
    "        y_predict = [x.predict_proba(x_1d_test_array) for x in n_estimator]\n",
    "        if(is_debug):\n",
    "            print(f'y_predict = {y_predict}')\n",
    "        #return the probability that is 1(left wins)     \n",
    "        total_1_prob = [x[0][1] for x in y_predict]\n",
    "        ret_val = np.mean(total_1_prob)\n",
    "    \n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TotalModel(**all_classifiers_dic):\n",
    "    level_classifier_dic_list = []\n",
    "    final_classifier = {}\n",
    "    for name, classifier_dic in all_classifiers_dic.items():\n",
    "        level_classifier_dic_list.append(classifier_dic)\n",
    "    \n",
    "    final_classifier = level_classifier_dic_list[-1]\n",
    "    del level_classifier_dic_list[-1]\n",
    "    \n",
    "    for classifier_dic in level_classifier_dic_list:\n",
    "        for name, classifier in classifier_dic.items():\n",
    "            print(f'--->{name} = {classifier}')\n",
    "            \n",
    "    for name, classifier in final_classifier.items():\n",
    "        print(f'--->{name} = {classifier}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFile = '../../python_ex/NBA_Ino_Part/nba_preprocessed.csv'\n",
    "dateStart = '2017-08-01'\n",
    "dateEnd = '2018-05-01'\n",
    "# X, Y = featureExtraction(dfFile, dateStart, dateEnd)\n",
    "X, Y = featureExtraction(dfFile, attriToDrop=['PTS'], dateStart=dateStart, dateEnd=dateEnd)\n",
    "# X, Y = featureExtraction(dfFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--->logistic_classifier = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "--->svm_classifier = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "--->naive_bayse_gaussian_classifier = GaussianNB(priors=None)\n",
      "--->rf_classifer = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "--->gbdt_classifier = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "--->xgb_classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "--->lgb_classifier = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
      "        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "        subsample_for_bin=200000, subsample_freq=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier \n",
    "\n",
    "level_1_classifier  = {'logistic_classifier'             : LogisticRegression(),\n",
    "                       'svm_classifier'                  : SVC(),\n",
    "                       'naive_bayse_gaussian_classifier' : GaussianNB()}\n",
    "\n",
    "level_2_classifier  = {'rf_classifer'    : RandomForestClassifier(),\n",
    "                       'gbdt_classifier' : GradientBoostingClassifier(),\n",
    "                       'xgb_classifier'  : XGBClassifier()}\n",
    "\n",
    "final_classifier    = {'lgb_classifier'  : LGBMClassifier()}\n",
    "\n",
    "all_classifiers_dic = {'level_1_classifier' : level_1_classifier,\n",
    "                       'level_2_classifier' : level_2_classifier,\n",
    "                       'final_classifier'   : final_classifier}\n",
    "\n",
    "TotalModel(**all_classifiers_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
